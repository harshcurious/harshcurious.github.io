<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML on harshcurious</title><link>https://harshcurious.com/tags/ml/</link><description>Recent content in ML on harshcurious</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 17 Mar 2021 14:59:43 +0530</lastBuildDate><atom:link href="https://harshcurious.com/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Kolmogorov Arnold Representation Theorem</title><link>https://harshcurious.com/posts/mathneuralnetwork/</link><pubDate>Wed, 17 Mar 2021 14:59:43 +0530</pubDate><guid>https://harshcurious.com/posts/mathneuralnetwork/</guid><description>The most surprizing aspect of neural networks is their simplicity. I don&amp;rsquo;t mean that the whole of a neural network is simple. It is not. But at any given instant you are either adding numbers or applying a function on one number (a single variable function). Why these two kinds of operations will give you any old function of multiple variables is a mystry to me! Let me take a small step towards understanding this by reading this really old paper by Kolmogrov.</description></item></channel></rss>